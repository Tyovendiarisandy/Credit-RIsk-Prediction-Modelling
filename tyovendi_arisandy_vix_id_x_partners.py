# -*- coding: utf-8 -*-
"""Tyovendi Arisandy_VIX_ID/X Partners.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U16QC0aki85taY9pWX4mua-ID-acdDks

#**Credit Risk Project**
##**ID/X Partners Data Scientist Virtual Internship Program Promoted by Rakamin Academy**

Created by: Tyovendi Arisandy

**List of Contents:**
- Business Understanding
- Analytic Approach
- Data Understanding
- Data Preparation
- Exploratory Data Analysis (EDA)
- Model Building
- Model Evaluation
- Conclusion
- Recommendation

---

##**Business Understanding**
####**Business Context**
Kata kredit berasal dari bahasa Latin Credere yang berarti percaya atau to believe atau to trust. Karenanya dasar pemikiran pemberian kredit oleh suatu perbankan kepada seseorang/lembaga adalah berdasarkan kepercayaan (faith). Sesuai Undang-Undang No. 10 Tahun 1998 tentang Perbankan, Kredit adalah penyediaan uang atau tagihan yang dapat dipersama-kan dengan itu, berdasarkan persetujuan atau kesepakatan pinjam-meminjam antara bank dengan pihak lain yang mewajibkan pihak peminjam untuk melunasi utangnya setelah jangka waktu tertentu dengan pemberian bunga. akan tetapi seiring berjalannya waktu, seringkali pihak pemberi kredit/pinjaman mengalami kerugian karena terjadinya kredit yang macet ataupun gagal bayar. untuk itu perlu adanya mekanisme yang lebih efektif untuk mengatasi permasalahan risiko kredit.

####**Problem Statment**
Risiko kredit (credit risk) didefinisikan sebagai risiko kerugian yang terkait dengan kemungkinan kegagalan counterparty memenuhi kewajibannya; atau risiko bahwa debitur tidak membayar kembali utangnya. Risiko kredit timbul dari adanya kemungkinan bahwa kredit yang diberikan oleh bank, atau obligasi yang dibeli, tidak dapat dibayarkan kembali. Risiko kredit juga timbul dari tidak dipenuhinya berbagai bentuk kewajiban pihak lain kepada bank, seperti kegagalan memenuhi kewajiban pembayaran dalam kontrak derivatif. Untuk sebagian bank, risiko kredit merupakan risiko terbesar yang dihadapi. Pada umumnya, marjin yang diperhitungkan untuk mengantisipasi risiko kredit hanyalah merupakan bagian kecil dari total kredit yang diberikan bank dan oleh karenanya kerugian pada kredit dapat menghancurkan modal bank dalam waktu singkat.

####**Goals**
Berdasarkan permasalahan diatas, mengurangi tingkat risiko kredit (credit risk) merupakan tujuan bisnis utama perusahaan. Maka dari itu, perlu adanya sebuah mekanisme yang dapat memprediksi apakah pihak peminjam termasuk kedalam kategori 'Bad' atau 'Good' dalam sistem kredit,  serta faktor-faktor yang memberikan informasi tambahan dan mendukung status 'Bad' atau 'Good' pihak peminjam dalam kasus credit risk ini.

---

##**Analytical Approach**
- Menganalisis data untuk dapat menemukan pola dari fitur-fitur yang tersedia dalam dataset, pada akhirnya menemukan karakteristik dari peminjam yang berstatus 'Bad' atau 'Good'.

- Membangun suatu model klasifikasi yang akan membantu perusahaan untuk dapat menyediakan 'tool' yang dapat memprediksi peminjam/debitur memperoleh status 'Bad' atau 'Good.

---

##**Data Understanding**
**Note:**
- Dataset yang diperoleh melalui e-learning rakamin academy dalam program **Data Scientist Virtual Internship Program** yang bekerja sama dengan perusahaan ID/X Partners
- Dataset yang digunakan merupakan **data perbankan** mengenai **credit risk** berdasarkan aktivitas transaksi customers bank tersebut dari tahun 2007-2014.

**Attribute Information:**

| Attributes | Data Type | Description |
|---|---|---|
| id | int64 |ID yang ditetapkan LC unik untuk daftar pinjaman|
| member_id | int64 |ID yang ditugaskan LC yang unik untuk anggota peminjam|
| loan_amnt | int64 |Pembayaran bulan lalu yang telah diterima|
| funded_amnt | int64 | Jumlah total yang diberikan untuk pinjaman pada saat itu |
| funded_amnt_inv | float64 | jumlah yang diberikan investor untuk peminjam | 
| term | object | Jumlah pembayaran pinjaman. Nilai dalam bulan dan dapat berupa 36 atau 60 |
| int_rate | float64 | Suku bunga |
| installment | float64 | Pembayaran bulanan terutang oleh peminjam jika pinjaman telah disetujui/diproses |
| grade | object | LC diberikan peringkat pinjaman |
| sub_grade | object | LC ditugaskan Subgrade Pinjaman |
| emp_title | object | Jabatan peminjam dalam pekerjaan Peminjam saat mengajukan pinjaman |
| emp_length | object | Lama kerja dalam tahun. Nilai yang mungkin antara 0 dan 10 di mana 0 berarti kurang dari satu tahun dan 10 berarti sepuluh tahun atau lebih |
| home_ownership | object | Status kepemilikan rumah yang diberikan oleh peminjam pada saat pendaftaran. Nilai-nilai kami adalah: SEWA, SENDIRI, KPR, LAINNYA |
| annual_inc | float64 | Pendapatan tahunan yang dilaporkan sendiri yang diberikan oleh peminjam selama pendaftaran |
| verification_status | object | Status yang telah diverifikasi oleh LC kepada Peminjam |
| issue_d | object | Bulan di mana pinjaman itu didanai |
| loan_status | object | Status pinjaman saat ini |
| pymnt_plan | object | Menunjukkan jika rencana pembayaran telah dibuat untuk pinjaman |
| url | object | URL untuk halaman LC dengan data daftar |
| desc | object | Deskripsi pinjaman yang diberikan oleh peminjam |
| purpose | object | Kategori yang disediakan oleh peminjam untuk permintaan pinjaman |
| title | object | Judul pinjaman yang diberikan oleh peminjam |
| zip_code | object | 3 angka pertama kode pos diberikan oleh peminjam dalam aplikasi pinjaman |
| addr_state | object | Negara yang disediakan oleh peminjam dalam aplikasi pinjaman |
| dti | float64 | rasio antara pinjaman dengan income |
| delinq_2yrs | float64 | Jumlah insiden tunggakan 30+ hari lewat jatuh tempo dalam file kredit peminjam selama 2 tahun terakhir |
| earliest_cr_line | object | Bulan dimana batas kredit paling awal yang dilaporkan peminjam dibuka |
| inq_last_6mths | float64 | Jumlah pertanyaan dalam 6 bulan terakhir (tidak termasuk pertanyaan mobil dan hipotek) |
| mths_since_last_delinq | float64 | Jumlah bulan sejak tunggakan terakhir peminjam |
| mths_since_last_record | float64 | Jumlah bulan sejak catatan publik terakhir |
| open_acc | float64 | Jumlah jalur kredit terbuka dalam file kredit peminjam |
| pub_rec | float64 | Jumlah catatan publik yang menghina |
| revol_bal | int64 | Total saldo revolving credit |
| revol_util | float64 | Tingkat pemanfaatan jalur bergulir, atau jumlah kredit yang digunakan peminjam relatif terhadap semua kredit revolving yang tersedia |
| total_acc | float64 | Jumlah total jalur kredit saat ini dalam file kredit peminjam |
| initial_list_status | object | Status pencatatan awal pinjaman. Nilai yang mungkin adalah â€“ Utuh, Pecahan |
| out_prncp | float64 | Sisa pokok pinjaman untuk jumlah total yang didanai |
| out_prncp_inv | float64 | Sisa pokok pinjaman untuk sebagian dari jumlah total yang didanai oleh investor |
| total_pymnt | float64 | Pembayaran diterima hingga saat ini untuk jumlah total yang didanai |
| total_pymnt_inv | float64 | Pembayaran yang diterima hingga saat ini untuk sebagian dari jumlah total yang didanai oleh investor |
| total_rec_prncp | float64 | Prinsipal yang diterima hingga saat ini |
| total_rec_int | float64 | Bunga yang diterima hingga saat ini |
| total_rec_late_fee | float64 | Biaya keterlambatan diterima hingga saat ini |
| recoveries | float64 | Menunjukkan jika rencana pembayaran telah diberlakukan untuk pinjaman |
| collection_recovery_fee | float64 | biaya pengumpulan post charge off |
| last_pymnt_d | object | Pembayaran bulan lalu telah diterima |
| last_pymnt_amnt | float64 | Jumlah total pembayaran terakhir yang diterima |
| next_pymnt_d | object | Tanggal pembayaran terjadwal berikutnya |
| last_credit_pull_d | object | LC bulan terakhir menarik kredit untuk pinjaman ini |
| collections_12_mths_ex_med | float64 | Jumlah koleksi dalam 12 bulan tidak termasuk koleksi medis |
| mths_since_last_major_derog | float64 | Bulan sejak peringkat 90 hari atau lebih buruk terakhir |
| policy_code | int64 | policy_code yang tersedia untuk umum=1, produk baru tidak tersedia untuk umum policy_code=2 |
| application_type | object | Menunjukkan apakah pinjaman tersebut merupakan aplikasi individu atau aplikasi bersama dengan dua peminjam bersama |
| annual_inc_joint | float64 | Gabungan pendapatan tahunan yang dilaporkan sendiri yang disediakan oleh peminjam bersama selama pendaftaran |
| dti_joint | float64 | Rasio yang dihitung menggunakan total pembayaran bulanan peminjam bersama atas total kewajiban utang, tidak termasuk hipotek dan pinjaman LC yang diminta, dibagi dengan pendapatan bulanan gabungan yang dilaporkan sendiri oleh peminjam bersama |
| verification_status_joint | float64 | Menunjukkan jika pendapatan bersama co-peminjam diverifikasi oleh LC, tidak diverifikasi, atau jika sumber pendapatan diverifikasi |
| acc_now_delinq | float64 | Jumlah rekening di mana peminjam sekarang menunggak |
| tot_coll_amt | float64 | Total pengumpulan yang pernah terutang |
| tot_cur_bal | float64 | Total saldo saat ini dari semua akun |
| open_acc_6m | float64 | Jumlah perdagangan terbuka dalam 6 bulan terakhir |
| open_il_6m | float64 | Jumlah rekening angsuran yang dibuka dalam 6 bulan terakhir |
| open_il_12m | float64 | Jumlah rekening angsuran yang dibuka dalam 12 bulan terakhir |
| open_il_24m | float64 | Jumlah rekening angsuran yang dibuka dalam 24 bulan terakhir |
| mths_since_rcnt_il | float64 | Bulan sejak akun angsuran terakhir dibuka |
| total_bal_il | float64 | Total saldo saat ini dari semua akun angsuran | 
| il_util | float64 | Rasio total saldo saat ini terhadap kredit/batas kredit yang tinggi pada semua akun angsuran |
| open_rv_12m | float64 | Jumlah perdagangan bergulir (revolving trade) yang dibuka dalam 12 bulan terakhir |
| open_rv_24m | float64 | Jumlah perdagangan bergulir (revolving trade) yang dibuka dalam 24 bulan terakhir |
| max_bal_bc | float64 | Saldo maksimum saat ini terutang pada semua akun bergulir (revolving account) |
| all_util | float64 | Saldo ke batas kredit (credit limit) pada semua perdagangan |
| total_rev_hi_lim | float64 | Total revolving high credit/credit limit
| inq_fi | float64 | Jumlah pertanyaan keuangan pribadi |
| total_cu_tl | float64 | Jumlah Perdagangan Keuangan |
| inq_last_12m | float64 | Jumlah pertanyaan kredit dalam 12 bulan terakhir |

---

##**Data Preparation**

###**1) Import Library**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

"""###**2) Load & Explore Dataset**

"""

# load dataset with pandas
df = pd.read_csv('/content/drive/MyDrive/7.Virtual Internship/Id x Partners/Dataset/loan_data_2007_2014.csv')

# set to show maximum columns
pd.set_option('max_columns', None)

# check 5 upper data from the dataset
df.head()

# check 5 lower data from the dataset
df.tail()

"""###**3) Defining the Target/Label Column to Identify Bad Loans and Good Loans**

- Karena kita ingin memprediksi apakah suatu pinjaman akan berisiko atau tidak maka perlu untuk membuat klasifikasi dari riwayat/histori transaksi pinjaman sebelumnya. apakah pinjaman **gagal bayar/penutupan akun (default/current off), atau telah lunas (fully paid)**. Dalam kasus ini, kita lebih menekankan pada klasifikasi **Bad Loans** untuk menghindari kerugian perusahaan.

- Adapun klasifikasi pinjaman baik (Good Loans) dan pinjaman berisiko (Bad Loans) yakni sebagai berikut:
```
bad loans = ["Charged Off", "Late (31-120 days)", "Late (16-30 days)", "Default", "Does not meet the credit policy. Status:Charged Off"]
```
```
good_loans = ['Current', 'Fully Paid', 'In Grace Period', 'Does not meet the credit policy. Status:Fully Paid']
```
"""

# list of bad loans categories
bad_loans = ["Charged Off", "Late (31-120 days)", "Late (16-30 days)", 
             "Default", "Does not meet the credit policy. Status:Charged Off"]

df['status'] = np.where(df['loan_status'].isin(bad_loans),1,0)

# drop loan status
df = df.drop(['loan_status'], axis=1)

"""###**4) Data Cleaning**"""

# check information of the dataset
df.info()

# check number of rows and columns dataset
df.shape

# check the dataset if any the missing values

checker = []
for col in df.columns :
    checker.append([col, df[col].dtype, 
                     df[col].isnull().sum(), 
                     round((df[col].isnull().sum()/len(df[col])) * 100,2),
                     df[col].nunique()]);

df_info = pd.DataFrame(data=checker, columns=['Features', 'Data_Type', 'Missing_Value', 'Missing_Value_Pct', 'Unique_Value'])

df_info.head(60)

df_info.tail(15)

"""---
> Menurut **Little & Rubin** (1987) menyatakan bahwa terdapat 3 jenis/tipe dari missing value, diantaranya:
- ***Missing Completely at Random (MCAR)***: berarti bahwa tidak ada hubungan antara apakah titik data hilang dan nilai dalam kumpulan data, hilang atau diamati. Adapun data yang hilang hanyalah subset acak dari data.
- ***Missing at Random (MAR)***: berarti bahwa kecenderungan nilai yang hilang memiliki hubungan sistematis dengan data yang diamati tetapi tidak dengan data yang hilang. hal ini juga menunjukan terjadinya missing data hanya berkaitan dengan variabel respon/pengamatan.
- ***Missing Not at Random (MNAR)***: Mekanisme data hilang yang tidak terdistribusi secara random. Dengan kata lain, Missingness Is Non-Ignorable bahwa terjadinya missing data pada suatu variabel berkaitan dengan variabel itu sendiri, sehingga ini tidak bisa diprediksi dari variabel lain pada suatu dataset. Dengan begitu apabila MNAR terjadi dalam persentase yang cukup besar perlu dipastikan apakah data yang terjadi MNAR tersebut akan diproses atau didrop.

Referensi:
- https://www.dqlab.id/kursus-belajar-data-mengenal-apa-itu-missing-value
- https://www.analyticsvidhya.com/blog/2021/10/handling-missing-value/
- Little, R.J.A., & D. B. Rubin. (1987). Statistical Analysis with Missing Data. New York: John Wiley & Sons.

---
"""

# saving missing values in a features
a = df.isnull().sum()/len(df)*100

# saving column names in a features
feature= df.columns

# features to drop variables having missing values more than a threshold

features = []
for i in range(df.columns.shape[0]):
    if a[i]>=50: #setting the threshold as 50%
        features.append(feature[i])

features

"""---
> **Observasi**:
>
> - Terdapat 17 kolom yang terjadi missing value, dimana persentase missing valuenya yaitu **100%**, diantaranya:
``` 
annual_inc_joint, dti_joint, verification_status_joint, open_acc_6m, open_il_6m, open_il_12m, open_il_24m, mths_since_rcnt_il, total_bal_il, il_util, open_rv_12m, open_rv_24m, max_bal_bc, all_util, inq_fi, total_cu_tl, inq_last_12m 
```
>  - 17 kolom ini akan **didrop** karena tidak dapat dianalisa (tidak mengandung informasi apapun). 
>
> - Kolom berikut: 
```
desc, mths_since_last_delinq, mths_since_last_record, mths_since_last_major_derog
``` 
  - kolom tersebut terdapat missing value bertipe ***Missing Completely at Random (MCAR)***. mengalami missing value bertipe MCAR (tidak terdapat relasi dari karakteristik sampel data yang diobservasi) maka perlu di pastikan kembali bahwa missing value ini terjadi karena kegagalan/kesalahan selama proses penginputan data dan perlu dikonfirmasi ulang ke data engineer apakah memang sistem atau user yang menyebabkan terjadi missing value tersebut.
>  - persentase missing value **> 50%** maka lebih baik kolom tersebut didrop.
> - kolom ID atau memiliki high cardinality (memiliki sangat banyak unique value) juga akan di drop, diantaranya:
```
Unnamed: 0, id, member_id, pymnt_plan
```
> - Kolom yang tidak relevan (tidak diperlukan untuk proses analisa) juga akan didrop, diantaranya:
```
mths_since_last_delinq, mths_since_last_record,mths_since_last_major_derog, emp_title, title, next_pymnt_d,collection_recovery_fee, recoveries, total_rec_prncp,total_rec_late_fee, zip_code, sub_grade
```
> - Kolom yang hanya kurang dari 2 unique value juga akan didrop, diantaranya:
```
application_type, policy_code
```
> - Kolom yang berisi free text juga akan didrop, diantaranya:
```
url, desc
```
> - Kolom yang terindikasi mengalami data leakage (setelah dikeluarkanya pinjaman) juga akan didrop, diantaranya:
```
issue_d, loan_status, pymnt_plan, out_prncp, out_prncp_inv, total_pymnt, total_pymnt_inv, total_rec_prncp, total_rec_int, total_rec_late_fee, recoveries, collection_recovery_fee, last_pymnt_d, last_pymnt_amnt, next_pymnt_d
```
>  - Kolom yang berisikan data leakage menjadi sebuah permasalahan bagi machine learning (ML) untuk dapat memprediksi secara realistis. Karena kolom yang tergolong kedalam data leakage ini berisikan data/values setelah proses berlangsungnya transaksi pinjaman/kredit (ada sebelum prediksi dimulai). Oleh karena itu, hal tersebut bertentangan dengan tujuan awal dimana kita ingin membantu perusahaan dalam memprediksi apakah pinjaman akan beresiko atau tidak sebelum pinjaman diberikan, bukan setelah pinjaman diberikan (referensi bacaan: https://www.analyticsvidhya.com/blog/2021/07/data-leakage-and-its-effect-on-the-performance-of-an-ml-model/)
>  - Kolom dengan data leakage akan menyebabkan prediksi akan menjadi sangat akurat hanya dengan menggunakan kolom tersebut. contoh pada kolom **'out_prncp (outstanding principal/sisa pinjaman pokok yang belum terbayar oleh debitur)'**, jika menunjukan **angka 0** maka pinjaman telah lunas. Dengan begitu, dengan hanya menggunakan kolom ini kita sudah dapat menentukan bahwa peminjam sangat besar probabilitasnya untuk distatuskan menjadi 'good loan'. sehingga sangat mudah untuk memprediksi apakah peminjam berstatus 'bad loan' atau 'good loan' dengan hanya dengan menggunakan kolom tersebut. kolom-kolom tersebut tersedia karena proses transaksi kredit yang sudah berjalan, sehingga tidak akan ada kolom tersebut sebelum proses transaksi kredit berlangsung.
---
"""

# list any columns must be drop in this case

columns_to_drop = [# 100% missing value/ totally null
                   'annual_inc_joint','dti_joint','verification_status_joint',
                   'open_acc_6m','open_il_6m','open_il_12m','open_il_24m',
                   'mths_since_rcnt_il','total_bal_il','il_util','open_rv_12m',
                   'open_rv_24m','max_bal_bc','all_util','inq_fi','total_cu_tl',
                   'inq_last_12m',

                   # unique ID columns/high cardinality
                   'Unnamed: 0','id','member_id',

                   # only 1 value in column
                   'application_type','policy_code',

                   # free text
                   'url', 'desc',

                   # irrelevant columns
                   'mths_since_last_delinq','mths_since_last_record',
                   'mths_since_last_major_derog', 'emp_title','title','next_pymnt_d',
                   'collection_recovery_fee','recoveries','total_rec_prncp',
                   'total_rec_late_fee','zip_code','sub_grade',
                   
                   # data leakage
                   'issue_d','out_prncp', 'out_prncp_inv','pymnt_plan',
                   'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 
                   'total_rec_int', 'total_rec_late_fee', 'recoveries', 
                   'collection_recovery_fee', 'last_pymnt_d', 'last_pymnt_amnt', 
                   'next_pymnt_d']

# drop columns 
df = df.drop(columns= columns_to_drop, axis=1)

sum_null = df.isna().sum()
only_null = sum_null[sum_null > 0].sort_values(ascending=False)
pct_null = round(df.isna().sum()/len(df) * 100,2)
result = pd.concat([only_null,pct_null],1)
result.columns = ["missing_count","missing_ratio"]
print (result)

a = df.isna().sum()
a[a>0].sort_values(ascending=False).index

"""####**Impute Missing Value**"""

# list columns to fill NaN/Null with 0
fill_cols = ['tot_coll_amt','tot_cur_bal','total_rev_hi_lim']

for col in df[fill_cols]:
    df[col] = df[col].fillna(0)

# drop rows with NaN/Null (missing value < 5%)
drop_cols = ['revol_util', 'collections_12_mths_ex_med',
       'last_credit_pull_d', 'delinq_2yrs', 'earliest_cr_line',
       'inq_last_6mths', 'open_acc', 'pub_rec', 'total_acc', 'acc_now_delinq',
       'annual_inc']

for i in df[drop_cols]:
     df = df.dropna().reset_index(drop=True)

df.isna().sum()

"""###**5) Check the Column Indicated as Duplicated/Redundant**"""

df[['loan_amnt','funded_amnt','funded_amnt_inv']]

df[['loan_amnt','funded_amnt','funded_amnt_inv']].describe()

df[['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']]

df[['tot_coll_amt', 'tot_cur_bal', 'total_rev_hi_lim']].describe()

df.tot_coll_amt.value_counts(normalize=True)*100

df.tot_cur_bal.value_counts(normalize=True)*100

df.total_rev_hi_lim.value_counts(normalize=True)*100

plt.figure(figsize=(10,6))

sns.kdeplot(data=df[(df['tot_coll_amt'] > 0) & (df['tot_coll_amt'] < 10000000)], x= 'tot_coll_amt', hue='status')
plt.show()

plt.figure(figsize=(10,6))

sns.kdeplot(data=df[(df['tot_cur_bal'] > 0) & (df['tot_cur_bal'] < 10000000)], x= 'tot_cur_bal', hue='status')
plt.show()

plt.figure(figsize=(10,6))

sns.kdeplot(data=df[(df['total_rev_hi_lim'] > 0) & (df['total_rev_hi_lim'] < 10000000)], x= 'total_rev_hi_lim', hue='status')
plt.show()

"""> **Observasi:**
> - Kolom ```'loan_amnt','funded_amnt','funded_amnt_inv'``` memiliki karakteristik yang sama. Maka dari itu, hanya satu kolom saja yang dikeep untuk digunakan, sedangkan kolom lain akan kita drop.
> - Kolom ```'tot_coll_amt'``` memiliki 89% nilai 0.
> - Kolom ```'tot_cur_bal', 'total_rev_hi_lim'``` memiliki 15% (70276) nilai missing value dari keseluruhan data yang ada.
> - Missing value pada kolom ```'tot_coll_amt','tot_cur_bal', 'total_rev_hi_lim'``` telah dikonversi dari NaN/Null menjadi 0. 
> - Missing value ini tidak dapat dilakukan imputasi dengan nilai mean/median, karena tidak ada pembeda yang jelas dari nilai Bad Loans dan Good Loans. jadi lebih baik kolom ```'tot_coll_amt','tot_cur_bal', 'total_rev_hi_lim'```didrop.
"""

# drop redundant/not clear columns
df = df.drop(['funded_amnt','funded_amnt_inv','tot_coll_amt','tot_cur_bal','total_rev_hi_lim'], axis=1)

"""###**6) Data Type Transformation**

**Terdapat beberapa kolom yang tidak memiliki tipe data yang sesuai, diantaranya:**
- term
- emp_length
- issue_d
- earliest_cr_line
- last_pymnt_d
- last_credit_pull_d 

**kolom ini harus diubah kedalam format/tipe data yang lebih relevan**
"""

def term_transform(df, column):
    df[column] = pd.to_numeric(df[column].str.replace(' months', ''))

def emp_length_tranform(df, column):
    df[column] = df[column].str.replace('\+ years', '')
    df[column] = df[column].str.replace('< 1 year', str(0))
    df[column] = df[column].str.replace(' years', '')
    df[column] = df[column].str.replace(' year', '')
    df[column] = pd.to_numeric(df[column])
    df[column].fillna(value = 0, inplace = True)

def date_time_transform(df, column):
    today_date = pd.to_datetime('2022-10-01')
    df[column] = pd.to_datetime(df[column], format = "%b-%y")
    df['mths_since_' + column] = round(pd.to_numeric((today_date - df[column]) / np.timedelta64(1, 'M')))
    df['mths_since_' + column] = df['mths_since_' + column].apply(lambda x: df['mths_since_' + column].max() if x < 0 else x)
    df.drop(columns = [column], inplace = True)

term_transform(df,'term')

print(df.term.unique())
print(df.term.dtype)

emp_length_tranform(df,'emp_length')

print(df.emp_length.unique())
print(df.emp_length.dtype)

date_time_transform(df,'last_credit_pull_d')
date_time_transform(df,'earliest_cr_line')

print(df.mths_since_last_credit_pull_d.dtype)
print(df.mths_since_earliest_cr_line.dtype)

"""---

##**Exploratory Data Analysis (EDA)**
"""

df_eda = df.copy()

"""Sebelum kita mengekplorasi/menganalisis data, alangkah baiknya jika kita mengelompokkan kolom berdasarkan jenisnya, yaitu:

- Kolom target/label:
  ```
  'status'
  ```
- Kolom dengan tipe kategorikal:
  ```
  'term', 'emp_length', 'grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status'
  ```
- Kolom dengan tipe numerical:
  ```
  'loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util', 'total_acc', 'collections_12_mths_ex_med','acc_now_delinq', 'mths_since_last_credit_pull_d', 'mths_since_earliest_cr_line'
  ```
- kolom berikut:
  ```
  'term', emp_length','status'
  ```
  - memiliki tipe data numerical setelah di lakukan transfomasi, akan tetapi sebenarnya kolom ini bertipe kategorikal.
"""

# target/label column
lab_cols = ['status']

# categorical columns
cat_cols = ['term','emp_length','grade','home_ownership','verification_status','purpose',
            'addr_state','initial_list_status']

# numerical columns
num_cols = ['loan_amnt','int_rate','installment','annual_inc','dti','delinq_2yrs',
            'inq_last_6mths','open_acc','pub_rec','revol_bal','revol_util','total_acc',
            'collections_12_mths_ex_med','acc_now_delinq','mths_since_last_credit_pull_d',
            'mths_since_earliest_cr_line']

lab_cols

cat_cols

num_cols

"""####**1) Statistical Summary (Central Tendency)**

#####**Categorical Columns**
"""

df_eda[['grade','home_ownership','verification_status','purpose','addr_state','initial_list_status']].describe().T

"""#####**Numerical Columns**"""

df_eda[num_cols].describe().T

"""####**2) Univariate Analysis**

#####**Target/Label Columns**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='status')
ax.set_ylim(0,450000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()), va='bottom', weight='bold')
    ax.annotate(percentage, (x, y), va='bottom', size = 10, weight='bold')
plt.show()

"""> **Observasi:**
> - Terdapat **11% (49247)** pinjaman dengan status **Bad Loans**, sedangkan sebanyak **88% (395552)** pinjaman berstatus **Good Loans**.
> - 0 = Good Loans
> - 1 = Bad Loans

#####**Categorical Columns**

######**term**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='term')
ax.set_ylim(0,400000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.15, p.get_height()), va='bottom', weight='bold')
    ax.annotate(percentage, (x, y), va='bottom', size = 10, weight='bold')
plt.show()

"""> **Observasi:**
> - Sebesar **72%** term pinjaman di **36 bulan** dan sisanya **28%** di **60 bulan**

######**emp_length**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='emp_length')
ax.set_ylim(0,200000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - **emp_length >= 10 tahun** memiliki persentase tertinggi Sebesar **33.7%** dan terendah yaitu **9 tahun (4.0%)**

######**grade**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='grade')
ax.set_ylim(0,200000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - **Grade B** memiliki persentase tertinggi yaitu **29.4%** dan **Grade G** memiliki persentase terendah sebesar **0.7%**.

######**home_ownership**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='home_ownership')
ax.set_ylim(0,300000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - **Mortgage/KPR** memiliki persentase tertinggi yaitu **50.8%** dan **OWN/Rumah Pribadi** memiliki persentase terendah sebesar **8.5%**.
> - **Other/None/Any = 0%** tidak ada yang memiliki kepemilikan rumah selain RENT, OWN, dan MORTGAGE.

######**verification_status**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='verification_status')
ax.set_ylim(0,300000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Terlihat verification_status menunjukan persentase yang sama besar yaitu sebesar **Â±30%**

######**purpose**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, 
                  y='purpose',
                  order= df_eda['purpose'].value_counts().index)
ax.set_xlim(0,300000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_width()/len(df_eda))
    x = p.get_x() + p.get_width() 
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Tujuan dari pinjaman/kredit yang memiliki persentase tertinggi yaitu **debt_consolidation (59.1%)**.

######**addr_state**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, 
                  y='addr_state',
                  order= df_eda['addr_state'].value_counts().head(10).index)
ax.set_xlim(0,80000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_width()/len(df_eda))
    x = p.get_x() + p.get_width() 
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Negara bagian **CA (California)** memiliki persentase tertinggi **(15.4%)** dari keseluruhan jumlah peminjam/kreditur.

######**initial_list_status**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='initial_list_status')
ax.set_ylim(0,350000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Sebesar **65%** status pendaftaran **penuh/lengkap (whole/w)** dan terdapat **35%** status pendaftaran yang **belum lengkap (fractional/f)**

#####**Numerical Columns**
"""

plt.figure(figsize=(15,10))
for i in range(0, len(num_cols)):
    plt.subplot(4, 4, i+1)
    sns.boxplot(x= df_eda[num_cols[i]], color='green')
    plt.xlabel(num_cols[i])
    plt.tight_layout()

"""> **Observasi:**
> - Rata-rata semua kolom numerical memiliki outliers

####**3) Multivariate Analysis**

#####**Categorical Columns**

######**term**
"""

plt.figure(figsize=(10,6))
ax= sns.countplot(data=df_eda, x='term', hue=df_eda['status'])
ax.set_ylim(0,400000)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.2, p.get_height()/2),va='top', ha='center',weight='bold')
    ax.annotate(percentage, (x, y), va='bottom', ha='center', size = 10, weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan pinjaman dengan ```term``` **36 bulan (7%)** memiliki **Bad Loans** lebih besar ketimbang pinjaman dengan ```term``` **60 bulan (4%)**.

######**emp_length**
"""

plt.figure(figsize=(10,6))
ax= sns.countplot(data=df_eda, x='emp_length', hue=df_eda['status'])
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan pinjaman **Bad Loans** dengan ```emp_length``` **>=10 tahun (3.4%)**
> - Rata-rata **Bad Loans** dengan ```emp_length``` **< 10 tahun** lebih rendah **Â± 1%**

######**grade**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='grade', hue=df_eda['status'])
ax.set_ylim(0,200000)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan **Bad Loans** dengan ```grade``` **C (3.1%) dan D (2.6%)** dan lebih sedikit di ```grade``` lainnya

######**home_ownership**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='home_ownership', hue=df_eda['status'])
ax.set_ylim(0,300000)
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan pinjaman **Bad Loans** yaitu ```home_ownership``` **RENT (5.2%) dan MORTGAGE (5.0%)** 
> - Pinjaman **Bad Loans** dengan ```home_ownership``` **OWN (0.9%)** cenderung lebih sedikit

######**verification_status**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, x='verification_status',hue=df_eda['status'])
ax.set_ylim(0,300000)
for p in ax.patches:
    percentage = '({:.1f}%)'.format(100 * p.get_height()/len(df_eda))
    x = p.get_x() + p.get_width() / 2
    y = p.get_y() + p.get_height() 
    ax.annotate(percentage, (x, y), size = 10, ha='center', va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan pinjaman **Bad Loans** hampir setara, tetapi ```verification_status``` yang **verified (4.4%)** lebih tinggi secara persentase dibanding lainnya

######**purpose**
"""

plt.figure(figsize=(12,8))
ax= sns.countplot(data=df_eda, 
                  y='purpose',
                  hue= 'status',
                  order= df_eda['purpose'].value_counts().index)
ax.set_xlim(0,280000)
ax.legend(title= 'status', bbox_to_anchor=(1.01,1.02), 
          loc='upper left', edgecolor='black')
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_width()/len(df_eda))
    x = p.get_x() + p.get_width() 
    y = p.get_y() + p.get_height() / 1
    ax.annotate(percentage, (x, y), size = 10, va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Kebanyakan pinjaman **Bad Loans** berdasarkan ```purpose``` yaitu **debt consolidation (6.8%)**, kemudian disusul **credit card (1,9%)**
> - Rata-rata **Bad Loans** hampir sama di ```purpose``` lainnya (selain debt consolidation & credit card)

######**addr_state**
"""

plt.figure(figsize=(12,8))
ax= sns.countplot(data=df_eda, 
                  y='addr_state',
                  hue= 'status',
                  order= df_eda['addr_state'].value_counts().head(10).index)
ax.set_xlim(0,280000)
ax.legend(title= 'status', bbox_to_anchor=(1.01,1.02), 
          loc='upper left', edgecolor='black')
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_width()/len(df_eda))
    x = p.get_x() + p.get_width() 
    y = p.get_y() + p.get_height() / 1
    ax.annotate(percentage, (x, y), size = 10, va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Negara bagian **CA (California)** menjadi negara dengan persentage **Bad Loans (1.8%)** terbanyak dari keseluruhan pinjaman yang ada

######**initial_list_status**
"""

plt.figure(figsize=(8,4))
ax= sns.countplot(data=df_eda, 
                  y='initial_list_status',
                  hue= 'status',
                  order= df_eda['initial_list_status'].value_counts().index)
ax.set_xlim(0,280000)
ax.legend(title= 'status', bbox_to_anchor=(1.01,1.02), 
          loc='upper left', edgecolor='black')
for p in ax.patches:
    percentage = '{:.1f}%'.format(100 * p.get_width()/len(df_eda))
    x = p.get_x() + p.get_width() 
    y = p.get_y() + p.get_height()/1.6
    ax.annotate(percentage, (x, y), size = 10, ha= 'left',va='bottom', weight='bold')
plt.show()

"""> **Observasi:**
> - Terlihat bahwa pinjaman Bad Loans berdasarkan ```initial_list_status``` yang kurang lengkap **(fractional/f)** lebih besar persentasenya **(7.9%)** daripada yang lengkap **(whole/w)** yaitu hanya sekitar **3.2%**

#####**Numerical Columns**
"""

loss_amnt = df_eda.groupby(['loan_amnt','status']).agg({'loan_amnt': [('total', 'sum')]})
loss_amnt.columns = loss_amnt.columns.droplevel()
loss_amnt = loss_amnt.sort_values('total', ascending=False)
loss_amnt.reset_index(inplace=True)
loss_amnt

loss_amnt[loss_amnt['status']== 1].sum()

loss_amnt[loss_amnt['status']== 0].sum()

# Total Kredit (Good Loan + Bad Loan)
5697873550+730682825

df_eda['loan_amnt'].sum()

interval_range = pd.interval_range(start=0, freq=10000, end=40000)
df_eda['loan_amnt_cat'] = pd.cut(df_eda['loan_amnt'], bins=interval_range)

plt.figure(figsize=(8,4))
ax = sns.countplot(data=df_eda,
                 x="loan_amnt_cat", 
                 hue="status")

df_eda = df_eda.drop(['loan_amnt_cat'], axis=1)

plt.figure(figsize=(15,20))
for i in range(0, len(num_cols)):
    plt.subplot(8, 4, i+1)
    sns.boxplot(y=df_eda[num_cols[i]], x=df_eda['status'])
    plt.xlabel(num_cols[i])
    plt.tight_layout()

"""> **Observasi:**
>
> - Total seluruh pinjaman yang diterbitkan yaitu **$6.428.556.375 B**

> - Total pinjaman yang diterbitkan untuk pinjaman yang berstatus Good Loans sebesar **$5.697.873.550 B**

> - Total pinjaman yang diterbitkan untuk pinjaman yang berstatus Bad Loans sebesar **$730.682.825 M**

> - Berdasarkan **ratio pinjaman** antara Good Loans vs Bad Loans, total pinjaman **Good Loans > Bad Loans**. akan tetapi potensi kerugian sebesar **$730.682.825 M** masih tetap akan terjadi jika peminjam/debitur tidak sanggup membayar utangnya/kredit dan berakhir pada kondisi gagal bayar (default)

> - Kebanyakan pinjaman berstatus Bad Loans terbanyak berada pada rentang **$10000 - 20000**

> **Observasi:**
> - Rata-rata ```loan_amnt``` atau jumlah pinjaman/kredit yang berstatus Bad Loans memiliki  rata-rata jumlah pinjaman lebih tinggi sebesar  lebih tinggi dari Good Loans. 
> - Terlihat bahwa```int_rate``` atau suku bunga untuk pinjaman dalam kategori Bad Loans memiliki  rata-rata lebih tinggi dari Good Loans. 
> - Rasio utang > pendapatan atau ```dti``` nampak lebih tinggi rata-ratanya untuk pinjaman yang berstatus Bad Loans

####**4) Risk Percentage of Bad Loans**

######**Categorical Columns Risk Percentage**
"""

def credit_risk_pct_chart(x):
    ratio = (df_eda.groupby(x)['status']
             .value_counts(normalize=True)
             .mul(100) 
             .rename('risk percentage (%)')
             .reset_index())

    sns.lineplot(data=ratio[ratio['status'] == 1], x=x, y='risk percentage (%)')
    plt.show()

credit_risk_pct_chart('term')

credit_risk_pct_chart('emp_length')

credit_risk_pct_chart('grade')

credit_risk_pct_chart('home_ownership')

credit_risk_pct_chart('verification_status')

risk_pct = (df_eda.groupby(['purpose'])['status']
         .value_counts(normalize=True)
         .mul(100) 
         .rename('risk percentage (%)')
         .reset_index())

plt.figure(figsize=(6,4))
ax= sns.lineplot(data=risk_pct[risk_pct['status'] == 1], 
                 x='purpose', 
                 y='risk percentage (%)')
plt.xticks(rotation = 90)  
plt.show()

risk_pct = (df_eda.groupby(['addr_state'])['status']
         .value_counts(normalize=True)
         .mul(100) 
         .rename('risk percentage (%)')
         .reset_index())

plt.figure(figsize=(10,6))
ax= sns.lineplot(data=risk_pct[risk_pct['status'] == 1], 
                 x='addr_state', 
                 y='risk percentage (%)')
plt.xticks(rotation = 90)  
plt.show()

credit_risk_pct_chart('initial_list_status')

"""######**Numerical Columns Risk Percentage**"""

credit_risk_pct_chart('int_rate')

credit_risk_pct_chart('dti')

credit_risk_pct_chart('acc_now_delinq')

credit_risk_pct_chart('inq_last_6mths')

credit_risk_pct_chart('mths_since_earliest_cr_line')

credit_risk_pct_chart('mths_since_last_credit_pull_d')

"""######**Insight**

> - `term`: semakin tinggi term maka semakin tinggi persentase risiko terjadinya Bad Loans
> - `emp_length`: semakin lama masa kerja peminjam/debitur maka persentase risiko terjadinya Bad Loans semakin kecil
> - `grade`: semakin tinggi grade pinjaman maka persentase risiko terjadinya Bad Loans semakin tinggi
> - `home_ownership`: jika peminjam/debitur tidak memiliki kepemilikan rumah (none/any/other) maka persentase risiko terjadinya Bad Loans sangat tinggi, sedangkan yang memiliki hipotik cenderung rendah terjadinya Bad Loans
> - `purpose`: tujuan pinjaman untuk bisnis kecil dan dana pendidikan memiliki persentase risiko terjadi Bad Loans sangat tinggi. sedangkan untuk kebutuhan kartu credit dan kredit kendaraan (mobil) cenderung rendah untuk terjadinya Bad Loans
> - `initial_list_status`: jika pinjaman telah memiliki berkas yang lengkap, maka persentase risiko terjadi Bad Loans cenderung rendah. sedangkan yang tidak melengkapi berkas pinjaman cenderung sangat tinggi untuk menaikan persentase risiko terjadinya Bad Loans
> - `int_rate`: kenaikan persentase risiko tejadinya Bad Loans dari suku bunga terlihat bervariasi, akan tetapi jika melihat trendnya, maka akan tampak bahwa semakin tinggi suku bunga maka akan semakin naik persentase risiko terjadinya Bad Loans 
> - `dti`: jika cicilan/angsuran rationya diatas 35, maka terlihat adanya kenaikan persentase risiko terjadinya Bad Loans yang signifikan
> - `acc_now_delinq`: jika akun rekening kredit > 2 akun, maka persentase risiko terjadinya Bad Loans naik secara signifikan

##**Feature Engineering**

###**1) Feature Encoding**
"""

# install category encoder
!pip install category_encoders

# categorical feature to encode

cat_code = ['home_ownership','verification_status','purpose','addr_state',
            'initial_list_status'] 

# Target encoding for all categorical features
from category_encoders import TargetEncoder

encoder = TargetEncoder()
for col in cat_code:
    df_eda[col]= encoder.fit_transform(df_eda[col], df_eda['status'])

from category_encoders import OrdinalEncoder

# Target encoding for ordinal features
encoder = OrdinalEncoder(cols=['grade'],return_df=True,
                           mapping=[{'col':'grade',
                                     'mapping':{'A': 1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7}}])

df_eda['grade'] = encoder.fit_transform(df_eda['grade'])

from category_encoders import OrdinalEncoder

# Target encoding for ordinal features
encoder = OrdinalEncoder(cols=['emp_length'],return_df=True,
                           mapping=[{'col':'emp_length',
                                     'mapping':{0:0, 1:1, 2:2, 3:3, 4:4, 5:5,
                                                6:6, 7:7, 8:8, 9:9, 10:10}}])

df_eda['emp_length'] = encoder.fit_transform(df_eda['emp_length'])

df_eda.head()

"""###**2) Check Multicollinearity**"""

# check correlation all features with heatmap
corr_ = df_eda.corr()
plt.figure(figsize=(16,12))
sns.heatmap(df_eda.corr(), annot=True, fmt='.2f')
plt.show()

"""> **Observasi:**
> - Dari heatmap diatas, terlihat kolom `loan_amnt` dan `installment` terjadi **multicollinearity (> 0.8)**. begitu pula dengan kolom `int_rate` dengan `grade`

"""

# Check multicolinearity with VIF Score
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif 
from statsmodels.tools.tools import add_constant

X = add_constant(df_eda)

vif_score = pd.DataFrame([vif(X.values, i) 
               for i in range(X.shape[1])],
               index=X.columns).reset_index()
vif_score.columns = ['feature','vif']
vif_score = vif_score.loc[vif_score.feature!='const']
vif_score

"""> **Observasi:**
> - Dari tabel VIF score, terlihat kolom `loan_amnt`= 53, `int_rate`= 11, `installment`= 45, `grade`= 11, melebihi threshold nilai VIF score yang diterima  (VIF score < 10)
> - kolom yang melebihi ketentuan VIF score yang diterima, maka akan didrop
> - kolom yang akan didrop yakni `installment` dan `grade`
"""

# drop multicollinearity feature
df_eda = df_eda.drop(['installment','grade'], axis=1)

# check again correlation all features with heatmap
corr_ = df_eda.corr()
plt.figure(figsize=(16,12))
sns.heatmap(df_eda.corr(), annot=True, fmt='.2f')
plt.show()

"""> - Sudah lebih baik, tidak ada lagi fitur yang mengalami multicollinearity

###**3) Handling Imbalance Dataset**
"""

from imblearn.over_sampling import SMOTE

#transform the dataset
oversample= SMOTE(random_state=42)

X = df_eda.drop(columns='status').to_numpy()
y = df_eda['status'].to_numpy()
y = y.reshape(len(y),) # sklearn's shape requirement
X_os, y_os = oversample.fit_resample(X, y)

"""###**4) Split Dataset (Train & Test Set)**"""

# split dataset
from sklearn.model_selection import train_test_split 

X_train, X_test, y_train, y_test = train_test_split(X_os, y_os, 
                                                    test_size = 0.20, 
                                                    random_state = 42)

"""###**5) Feature Scaling**"""

# scaling the features
from sklearn.preprocessing import RobustScaler

scaler = RobustScaler()
X = scaler.fit_transform(X)

"""##**Machine Learning Modeling**

##**Benchmark Model**

###**1) Random Forest Classifier**
"""

# import model from sklearn
from sklearn.ensemble import RandomForestClassifier

#define model knn
rf_clf = RandomForestClassifier(random_state=42)
rf_clf.fit(X_train, y_train)

"""#####**Evaluation Random Forest Model**"""

# prepare prediction result on train data
y_train_pred_rf = rf_clf.predict(X_train)

# the confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(10,7))

cm = confusion_matrix(y_train, y_train_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(ax=ax)
plt.show()

# classification train data report
from sklearn.metrics import classification_report
y_train_pred_rf = rf_clf.predict(X_train)
print(classification_report(y_train, y_train_pred_rf))

# prepare prediction result on test data
y_test_pred_rf = rf_clf.predict(X_test)

# the confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(10,7))

cm = confusion_matrix(y_test, y_test_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(ax=ax)
plt.show()

# classification test data report
from sklearn.metrics import classification_report
y_test_pred_rf = rf_clf.predict(X_test)
print(classification_report(y_test, y_test_pred_rf))

"""###**2) XGBoost Classifier**"""

# import xgboost
import xgboost as xgb

# define model xgb
xgb_clf = xgb.XGBClassifier()
xgb_clf.fit(X_train, y_train)

"""#####**Evaluation XGBoost Model**"""

# prepare prediction result on train data
y_train_pred_xgb = xgb_clf.predict(X_train)

# the confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(10,7))

cm = confusion_matrix(y_train, y_train_pred_xgb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(ax=ax)
plt.show()

# classification train data report
from sklearn.metrics import classification_report
y_train_pred_xgb = xgb_clf.predict(X_train)
print(classification_report(y_train, y_train_pred_xgb))

# prepare prediction result on test data
y_test_pred_xgb = xgb_clf.predict(X_test)

# the confusion matrix
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

fig, ax = plt.subplots(figsize=(10,7))

cm = confusion_matrix(y_test, y_test_pred_xgb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot(ax=ax)
plt.show()

# classification test data report
from sklearn.metrics import classification_report
y_test_pred_xgb = xgb_clf.predict(X_test)
print(classification_report(y_test, y_test_pred_xgb))

"""###**3) AUC-ROC Metric from Models**"""

from sklearn.metrics import roc_curve, auc

rf_fpr, rf_tpr, threshold = roc_curve(y_test, y_test_pred_rf)
auc_rf = auc(rf_fpr, rf_tpr)

xgb_fpr, xgb_tpr, threshold = roc_curve(y_test, y_test_pred_xgb)
auc_xgb = auc(xgb_fpr, xgb_tpr)

plt.figure(figsize=(5, 5), dpi=100)
plt.plot(xgb_fpr, xgb_tpr, linestyle='-', label='XGB (AUC = %0.3f)' % auc_xgb)
plt.plot(rf_fpr, rf_tpr, marker='.', label='RF (AUC = %0.3f)' % auc_rf)

plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')

plt.legend()

plt.show()

"""##**Hyperparameter Tuning For Improve Models**

###**Tuning for Random Forest Model**
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import RandomizedSearchCV
# 
# parameters = {
#     'n_estimators': (10,20,30,40,50,60,70,80,100),
#     'max_depth':(1,2,3,4,5,6,7,8,9,10)
# }
# 
# # note: we use roc_auc
# rf_clf_randomcv = RandomizedSearchCV(rf_clf, parameters, cv=5, scoring='roc_auc')
# rf_clf_randomcv.fit(X_train, y_train)

"""###**Tuning for XGBoost Model**"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import RandomizedSearchCV
# 
# parameters = {
#     'learning_rate': [0.01, 0.10, 0.15, 0.20, 0.25, 0.30],
#     'max_depth': [2, 3, 5]
# }
# 
# # note: we use accuracy
# xgb_clf_randomcv = RandomizedSearchCV(xgb_clf, parameters, cv=5, scoring='accuracy')
# xgb_clf_randomcv.fit(X_train, y_train)

"""Karena Run time untuk melakukan hyperparameter tuning membutuhkan waktu yang sangat lama (> 30 menit), maka proses tuning untuk model akan di skip.

##**Feature Importance**
"""

#Check feature important from Random Forest Model
data_feature = df_eda.drop('status',axis=1)
feature_name_list=data_feature.columns
rf_clf.feature_names = feature_name_list
rf_df= pd.DataFrame({'feature': rf_clf.feature_names,'importance':rf_clf.feature_importances_})
rf_df

plt.figure(figsize=(9,5))
sns.barplot(x='importance', y='feature', data=rf_df.sort_values('importance', ascending=False), palette='mako')
plt.xticks(rotation=90);
plt.title('Feature Importance by Random Forest')
plt.xlabel('Feature Importance (%)', fontsize=16)
plt.xticks(rotation=0)
plt.ylabel('')
plt.show()

xgb_clf.feature_names = feature_name_list
feat_df= pd.DataFrame({'feature': xgb_clf.feature_names,'importance':xgb_clf.feature_importances_})

sorted_df=feat_df.sort_values('importance', ascending=False)

#Check feature important from XGBoost
plt.figure(figsize=(9,5))
sns.barplot(x='importance', y='feature', data=sorted_df, palette='mako')
plt.title('Feature Importance to predict price by XGBoost')
plt.xlabel('Feature Importance (%)',fontsize=16)
plt.ylabel('')
plt.show()